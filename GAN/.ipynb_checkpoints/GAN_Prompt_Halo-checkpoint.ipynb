{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 70000\n",
    "BATCH_SIZE = 256\n",
    "data_set = np.loadtxt(\"halo_data.csv\")\n",
    "x_train = data_set[:,0:81]\n",
    "X_Net = x_train.reshape(x_train.shape[0],9,9,1).astype('float32')\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X_Net).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(9*9*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((9, 9, 256)))\n",
    "    assert model.output_shape == (None, 9, 9, 256) # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (3, 3), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 9, 9, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (3, 3), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 9, 9, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (3, 3), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 9, 9, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAANZUlEQVR4nO3df4hd9ZnH8ffjTDTNaGLcrYs7kY1iiQxCVYK0ugQ3bpao1YIuotCKIgRkW3WpFLv/FP3LP9bQCqVGNN2FuoprFWvwR0UtpeJqE5NdfySKm7RNsmY1WZLJTGJi7LN/zE2Z2ow59+Z8czPfvl8wODP35vEz4ifnzJlzn4nMRFI9jut3AEntstRSZSy1VBlLLVXGUkuVGSwxdObMmTk0NFRiNABz584tNhvgwIEDRecD7Nmzp+j8E044oej8444rezz4+OOPi84HmDNnTtH5u3btKjZ7586d7NmzJw71WJFSDw0Ncfnll5cYDcDVV19dbDbA9u3bi84HWLt2bdH58+fPLzp/1qxZRedv27at6HyAyy67rOj85557rtjsFStWTPmYp99SZSy1VBlLLVXGUkuVsdRSZSy1VBlLLVWmUakjYmlEvBMR70XEHaVDSerdYUsdEQPAD4BLgRHguogYKR1MUm+aHKkvAN7LzI2ZuR94BPhq2ViSetWk1MPA5kkfb+l87g9ExLKIWB0Rq/ft29dWPkldau1CWWben5kLM3Nh6RcTSJpak1JvBU6f9PG8zuckHYOalPpXwBci4oyIOB64Fvhp2ViSenXYl15m5oGI+AbwHDAArMzMt4onk9STRq+nzsyngacLZ5HUAu8okypjqaXKWGqpMpZaqoyllipjqaXKFFkRvHfvXt54440SowG44ooris0GWLx4cdH5ADt27Cg6/7777is6/5NPPik6/6abbio6H+Dee+8tOv/CCy8sNntwcOrqeqSWKmOppcpYaqkyllqqjKWWKmOppcpYaqkyllqqTJMVwSsj4oOIePNoBJJ0ZJocqf8FWFo4h6SWHLbUmfkL4P+OQhZJLfB7aqkyrb2gIyKWAcsAZsyY0dZYSV0qssz/s15BIqksT7+lyjT5kdbDwCvAgojYEhHlX+gqqWdNlvlfdzSCSGqHp99SZSy1VBlLLVXGUkuVsdRSZSy1VJnIzNaHDg8P580339z63INGRkaKzQbYtGlT0fkAq1atKjp//vz5ReffcMMNRec/+eSTRecDjI2NFZ2/Zs2aYrM3bNjA+Ph4HOoxj9RSZSy1VBlLLVXGUkuVsdRSZSy1VBlLLVXGUkuVsdRSZZpsPjk9Il6KiLcj4q2IuPVoBJPUmyYbAg8A38rM1yPiJGBNRDyfmW8XziapB02W+b+fma933t8NrAeGSweT1JuuvqeOiPnAecCrh3hsWUSsjojV4+Pj7aST1LXGpY6IE4GfALdl5uinH5+893toaKjNjJK60KjUETGDiUI/lJmPl40k6Ug0ufodwIPA+sxcXj6SpCPR5Eh9EfB1YHFErOu8XVY4l6QeNVnm/0vgkBsWJB17vKNMqoyllipjqaXKWGqpMpZaqoyllirT5FVaXZs5cyYLFiwoMRqAV155pdhsgB07dhSdD7B48eKi8+fOnVt0/u233150/pIlS4rOBzjttNOKzr/zzjuLzV66dOmUj3mklipjqaXKWGqpMpZaqoyllipjqaXKWGqpMpZaqkyTzSczI+K1iPjPzt7vcj9Rl3TEmtxRtg9YnJljnV1lv4yIZzLzPwpnk9SDJptPEhjrfDij85YlQ0nqXdNtogMRsQ74AHg+Mz9z7/fo6B9tEJZ0lDQqdWZ+kpnnAvOACyLinEM85/d7v2fPnt12TkkNdXX1OzN3Ai8BU79ERFJfNbn6/fmIOLnz/ueAJcCG0sEk9abJ1e/TgH+NiAEm/hJ4NDNXlY0lqVdNrn7/FxO/FE/SNOAdZVJlLLVUGUstVcZSS5Wx1FJlLLVUmSJ7vwcHBzn11FNLjAZg0aJFxWYD3HPPPUXnA5x77rlF55999tlF559zzh/dKdyqa665puh8gLvuuqvo/H379hWbvX379ikf80gtVcZSS5Wx1FJlLLVUGUstVcZSS5Wx1FJlLLVUmcal7iwfXBsRLkiQjmHdHKlvBdaXCiKpHU1XBM8DLgceKBtH0pFqeqT+HvBt4HdTPWHy3u9du3a1Ek5S95psE/0K8EFmrvms503e+z1nzpzWAkrqTpMj9UXAlRHxa+ARYHFE/LhoKkk9O2ypM/M7mTkvM+cD1wIvZubXiieT1BN/Ti1VpqslCZn5c+DnRZJIaoVHaqkyllqqjKWWKmOppcpYaqkyllqqTJG932NjY7z88sslRgNQ+t7yjRs3Fp0PsG7duqLzX3vttaLzzzzzzKLz77777qLzAWbMmFF0/osvvlhs9u7du6d8zCO1VBlLLVXGUkuVsdRSZSy1VBlLLVXGUkuVsdRSZRrdfNJZZbQb+AQ4kJkLS4aS1Ltu7ij7m8yc+tfXSzomePotVaZpqRP4WUSsiYhlh3rC5L3f4+Pj7SWU1JWmp99/nZlbI+JU4PmI2JCZv5j8hMy8H7gfYHh4OFvOKamhRkfqzNza+ecHwBPABSVDSepdk9/QMRQRJx18H/g74M3SwST1psnp918AT0TEwef/W2Y+WzSVpJ4dttSZuRH44lHIIqkF/khLqoyllipjqaXKWGqpMpZaqoyllipTZO/3wMAAJ510UonRAGzbtq3YbIDNmzcXnQ+wcuXKovOfeeaZovPffffdovOvv/76ovMBbrzxxqLzL7744mKzBwenrq5HaqkyllqqjKWWKmOppcpYaqkyllqqjKWWKmOppco0KnVEnBwRj0XEhohYHxFfLh1MUm+a3lH2feDZzPz7iDgemFUwk6QjcNhSR8QcYBFwA0Bm7gf2l40lqVdNTr/PAD4EfhQRayPigc4Cwj8wee/32NhY60ElNdOk1IPA+cAPM/M8YBy449NPysz7M3NhZi488cQTW44pqakmpd4CbMnMVzsfP8ZEySUdgw5b6szcBmyOiAWdT10CvF00laSeNb36/U3goc6V741A2ReiSupZo1Jn5jrA30ktTQPeUSZVxlJLlbHUUmUstVQZSy1VxlJLlbHUUmUiM1sfetZZZ+Xy5ctbn3vQsmXLis0+GvMBSt8fPzIyUnT+3r17i85fsWJF0fkAixYtKjp/06ZNxWY/9dRTbN++PQ71mEdqqTKWWqqMpZYqY6mlylhqqTKWWqqMpZYqc9hSR8SCiFg36W00Im47GuEkde+wSxIy8x3gXICIGAC2Ak8UziWpR92efl8C/Hdm/qZEGElHrttSXws8XCKIpHY0LnVn6eCVwL9P8fjvl/mPjo62lU9Sl7o5Ul8KvJ6Z/3uoBycv8589e3Y76SR1rZtSX4en3tIxr+mvsh0ClgCPl40j6Ug13fs9DvxZ4SySWuAdZVJlLLVUGUstVcZSS5Wx1FJlLLVUGUstVabI3u958+blLbfc0vrcg8bGxorNBnjhhReKzge46qqris5/9NFHi86POOTK6daccsopRecDfPTRR8X/HaWsXr2a0dFR935LfwostVQZSy1VxlJLlbHUUmUstVQZSy1VxlJLlWm6+eQfI+KtiHgzIh6OiJmlg0nqTZPf0DEM3AIszMxzgAEmVgVLOgY1Pf0eBD4XEYPALOB/ykWSdCQOW+rM3Ar8M/Bb4H1gV2b+7NPPm7z3e3x8vP2kkhppcvo9F/gqcAbwl8BQRHzt08+bvPd7aGio/aSSGmly+v23wKbM/DAzP2ZiTfCFZWNJ6lWTUv8W+FJEzIqJ19tdAqwvG0tSr5p8T/0q8BjwOvBG58/cXziXpB41Xeb/XeC7hbNIaoF3lEmVsdRSZSy1VBlLLVXGUkuVsdRSZYrs/Y6ID4HfdPFH/hzY3nqQo8f8/Tfdv4Zu8/9VZn7+UA8UKXW3ImJ1Zi7sd45emb//pvvX0GZ+T7+lylhqqTLHSqmn+73k5u+/6f41tJb/mPieWlJ7jpUjtaSWWGqpMn0tdUQsjYh3IuK9iLijn1l6ERGnR8RLEfF2Z4Xyrf3O1IuIGIiItRGxqt9ZuhURJ0fEYxGxISLWR8SX+52pGyXWb/et1BExAPwAuBQYAa6LiJF+5enRAeBbmTkCfAn4h2n4NQDcyvTdZvN94NnMPBv4ItPo6yi1frufR+oLgPcyc2Nm7gceYWLB4bSRme9n5uud93cz8T/UcH9TdSci5gGXAw/0O0u3ImIOsAh4ECAz92fmzv6m6lrr67f7WephYPOkj7cwzQoxWUTMB84DXu1vkq59D/g28Lt+B+nBGcCHwI863z48EBHTZpVt0/Xb3fJCWQsi4kTgJ8BtmTna7zxNRcRXgA8yc02/s/RoEDgf+GFmngeMA9Pm2kzT9dvd6meptwKnT/p4Xudz00pEzGCi0A9l5uP9ztOli4ArI+LXTHz7szgiftzfSF3ZAmzpLMeEiQWZ5/cxT7eKrN/uZ6l/BXwhIs6IiOOZuEDw0z7m6VpnZfKDwPrMXN7vPN3KzO9k5rzMnM/Ef/8XM/OIjxRHS2ZuAzZHxILOpy4B3u5jpG4VWb/daJtoCZl5ICK+ATzHxFW/lZn5Vr/y9Ogi4OvAGxGxrvO5f8rMp/uY6U/NN4GHOgeGjcCNfc7TWGa+GhEH128fANbSwu2i3iYqVcYLZVJlLLVUGUstVcZSS5Wx1FJlLLVUGUstVeb/ATlpaVdJBu8kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "\n",
    "print(type(generated_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same',\n",
    "                                     input_shape=[9, 9, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3,3), strides=(1, 1), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00561854]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator's job is to tell a generated image from a real one. We define two trvial labels 1 and 0 to internally represent real and fake images.\n",
    "We then use a binary cross entropy loss function. Recall that a binary cross entropy takes in two parameters: y = the target, and y' = the internal prediction for the target. \n",
    "the real_loss and the fake_loss are defined as the binary cross entropy with the target (ones_like/zero_like produce a tensor of the same rank as the given tensor) and the real/generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator needs to improve its efficiency if it is to cheat the discriminator. Its loss is calculated against the fake output in order to improve its generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 1\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):  \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "           # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "          # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    " \n",
    "    fig = plt.figure()\n",
    "    plt.imshow(predictions[0, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAEqklEQVR4nO3dwW7MbRyG4Xe0TDWtIliIM2DrnB0TCyIhEgkSValq5jsBnZWf715c19Ikz7/Tzt036eK12e12C+i59X9/AcCfiROixAlR4oQocULU4b4Xnzx5Mvqn3Kurq8n5tdlsRvf/xTOOjo5G9y8uLkb3Dw4ORvfXWuvwcO/HOO/Tp09//BA5OSFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSovRd+/v79e/ThJycno/v/4n9Qm/4eTd/Jenp6Orr/7du30f211tput6P75+fno/s3cXJClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUXsvRd1sNqMP//z58+j+2dnZ6P5aa929e3d0//r6enR/+u7ghw8fju6vtdbHjx9H96d/xjdxckKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSovZdKX11djT78/v37o/uHh3vf3l9xeno6uv/06dPR/ffv34/un5+fj+7/i2c8fvx4dP8mTk6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghau/FrtP3yn79+nV0f7vdju6vtdbz589H91+9ejW6/+DBg9H9g4OD0f215j+n19fXo/s3cXJClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUZvdbnfji8+ePbv5xb/g8vJycn5dXFyM7q+11suXL0f3j4+PR/ePjo5G99++fTu6v9Zab968Gd2/c+fO6P6XL182f/p3JydEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQdbjvxZ8/f44+fN+duX/D4eHet/dXvH79enT/1q3Z358fPnwY3X/x4sXo/lpr3bt3b3T/+/fvo/s3cXJClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqL23Lk9f+jx9YfL19fXo/lprXV5eju4fHx+P7j969Gh0/927d6P7a83/DM7Ozkb3b+LkhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6L23lu73W5HH/7r16/R/ZOTk9H9tebv3v3x48fo/tHR0ej+9Pdnrfn3cPv27dH9mzg5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihKjNbrf7v78G4A+cnBAlTogSJ0SJE6LECVHihKj/ABTcdie8X3KgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46min 10s, sys: 43.2 s, total: 46min 54s\n",
      "Wall time: 6min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dataset, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
